# BGL3 Environment Configuration

# LLM Model Selection
# Options: "qwen2.5:32b" (best for coding, 19GB)
#          "llama3.1" (lighter, 4.9GB)
# If your system struggles with qwen2.5:32b, change to llama3.1
LLM_MODEL=qwen2.5:32b

# LLM Base URL
LLM_BASE_URL=http://localhost:11434

# Debug mode (set to 1 for verbose logging)
LLM_DEBUG=0

# Warmup settings
LLM_WARMUP_MAX_WAIT=45
LLM_CHAT_TIMEOUT=60
